{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "P-ABC-SMC-COVID19.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYSlvx1_8JfK"
      },
      "source": [
        "# Massively Parallel ABC-SMC\n",
        "\n",
        "Efficient use of hardware acceleration platforms such as GPUs have revolutionized the domain of machine learning, especially deep learning. Simulation-based inference methods used in a wide variety of scientific disciples, which are currently run on CPUs, have a similar potential of improvement when mapped to parallelized paradigm of hardware acceleration platforms. In this work we propose a new simulation-based inference algorithm that is uniquely developed to exploit parallelism. \n",
        "\n",
        "## Model\n",
        "\n",
        "In the work, we consider a stocastic epidemology model used to predict and analyze the case data of COVID-19. The model has an intractible likelihood function, hence we need to use likelihood free inference.\n",
        "\n",
        "For training the model, we use data from the Johns Hopkins Dataset. (https://github.com/CSSEGISandData/COVID-19). We use the daily case data of Confirmed, Recevered and Deaths for a certain country. The code below preprocesses the data. The data preprocessing code was adapted from https://github.com/imdevskp/covid_19_jhu_data_web_scrap_and_cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W50DYP0Q7_j6"
      },
      "source": [
        "# Prerequisites for downloading and processing data\n",
        "!pip install wget\n",
        "!pip install countryinfo\n",
        "!nvidia-smi\n",
        "!pip install gputil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRqNhZw5TGIF"
      },
      "source": [
        "# import libraries\n",
        "# ================\n",
        "\n",
        "# for date and time opeations\n",
        "from datetime import datetime, timedelta\n",
        "# for file and folder operations\n",
        "import os\n",
        "# for regular expression opeations\n",
        "import re\n",
        "# for listing files in a folder\n",
        "import glob\n",
        "# for getting web contents\n",
        "import requests \n",
        "# storing and analysing data\n",
        "import pandas as pd\n",
        "# for scraping web contents\n",
        "from bs4 import BeautifulSoup\n",
        "# to download data\n",
        "import wget\n",
        "# numerical analysis\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaoXjAJSn0Jn"
      },
      "source": [
        "# GPU profiling\n",
        "\n",
        "import GPUtil\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "class Monitor(Thread):\n",
        "    def __init__(self, delay):\n",
        "        super(Monitor, self).__init__()\n",
        "        self.stopped = False\n",
        "        self.delay = delay # Time between calls to GPUtil\n",
        "        self.start()\n",
        "\n",
        "    def run(self):\n",
        "        while not self.stopped:\n",
        "            GPUtil.showUtilization()\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "    def stop(self):\n",
        "        self.stopped = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzguoD75TTF6"
      },
      "source": [
        "# remove all existing csv files\n",
        "! rm *.csv\n",
        "\n",
        "# urls of the files\n",
        "urls = ['https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv', \n",
        "        'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv',\n",
        "        'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv']\n",
        "\n",
        "# download files\n",
        "for url in urls:\n",
        "    filename = wget.download(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc-UgLoLTW2L"
      },
      "source": [
        "# dataset\n",
        "conf_df = pd.read_csv('time_series_covid19_confirmed_global.csv')\n",
        "deaths_df = pd.read_csv('time_series_covid19_deaths_global.csv')\n",
        "recv_df = pd.read_csv('time_series_covid19_recovered_global.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85FS3Ry8TbGK"
      },
      "source": [
        "# extract dates\n",
        "dates = conf_df.columns[4:]\n",
        "\n",
        "# melt dataframes into longer format\n",
        "# ==================================\n",
        "conf_df_long = conf_df.melt(id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], \n",
        "                            value_vars=dates, var_name='Date', value_name='Confirmed')\n",
        "\n",
        "deaths_df_long = deaths_df.melt(id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], \n",
        "                            value_vars=dates, var_name='Date', value_name='Deaths')\n",
        "\n",
        "recv_df_long = recv_df.melt(id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], \n",
        "                            value_vars=dates, var_name='Date', value_name='Recovered')\n",
        "\n",
        "recv_df_long = recv_df_long[recv_df_long['Country/Region']!='Canada']\n",
        "\n",
        "print(conf_df_long.shape)\n",
        "print(deaths_df_long.shape)\n",
        "print(recv_df_long.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG_rbHOVTemA"
      },
      "source": [
        "full_table = pd.merge(left=conf_df_long, right=deaths_df_long, how='left',\n",
        "                      on=['Province/State', 'Country/Region', 'Date', 'Lat', 'Long'])\n",
        "full_table = pd.merge(left=full_table, right=recv_df_long, how='left',\n",
        "                      on=['Province/State', 'Country/Region', 'Date', 'Lat', 'Long'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqu7bzOzThWe"
      },
      "source": [
        "# Convert to proper date format\n",
        "full_table['Date'] = pd.to_datetime(full_table['Date'])\n",
        "\n",
        "# fill na with 0\n",
        "full_table['Recovered'] = full_table['Recovered'].fillna(0)\n",
        "\n",
        "# convert to int datatype\n",
        "full_table['Recovered'] = full_table['Recovered'].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3jyOTpzTkZ-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from countryinfo import CountryInfo\n",
        "\n",
        "def get_country_data(country):\n",
        "  \"\"\"\n",
        "  Method to obtain case data for a certain country.\n",
        "  Input(String): name of country\n",
        "  Output(tf.constant): case data of country from the day of 100 confirmed cases\n",
        "  \"\"\"\n",
        "  country_data = (full_table[full_table['Country/Region'] == country])[['Confirmed', 'Recovered', 'Deaths']]\n",
        "  Confirmed = tf.constant(country_data[['Confirmed']].values, dtype=tf.float32)\n",
        "  Recovered = tf.constant(country_data[['Recovered']].values, dtype=tf.float32)\n",
        "  Deaths = tf.constant(country_data[['Deaths']].values, dtype=tf.float32)\n",
        "  country_data_full = tf.squeeze(tf.stack([Confirmed, Recovered, Deaths]))\n",
        "  # find start point, when sum(A,R,D) = 100\n",
        "  start_date = tf.reduce_min(tf.where(tf.reduce_sum(country_data_full, axis=0) >= 100))\n",
        "  return country_data_full[:, start_date:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE88fS-1UCIE"
      },
      "source": [
        "# Select data of a certain country\n",
        "\n",
        "COUNTRY = 'Italy'\n",
        "\n",
        "country_data = get_country_data(COUNTRY)\n",
        "\n",
        "TRAIN_DAYS = 120\n",
        "TEST_DAYS = 150\n",
        "\n",
        "population = CountryInfo(COUNTRY).population()\n",
        "country_data_train = country_data[:,:TRAIN_DAYS]\n",
        "country_data_test = country_data[:,:TEST_DAYS]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUXYWDsYS0pw"
      },
      "source": [
        "# imports\n",
        "\n",
        "import numpy as np\n",
        "import time as time\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tfd = tfp.distributions\n",
        "# for GPU\n",
        "tf.device(\"/GPU:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzqP_ELMWG0A"
      },
      "source": [
        "## Model class and other helper functions used for plotting results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjUzfTN8WSiQ"
      },
      "source": [
        "# Some important constants\n",
        "\n",
        "num_samples = 100000 # level of parallelism\n",
        "\n",
        "max_runs = 1000 # max runs to run the algorithms for\n",
        "\n",
        "NUM_TRIALS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHnO3-ygXySx"
      },
      "source": [
        "# Model and other helper functions\n",
        "\n",
        "def plot_summary(summary):\n",
        "    x_axis = np.arange(country_data_test.shape[1])\n",
        "    # x_axis = np.arange(country_data_test[:,TRAIN_DAYS:].shape[1])\n",
        "    fig, axs = plt.subplots(3)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    A_mean = tfp.stats.percentile(summary[:, 0, :], 50, axis=0)\n",
        "    A_min = tfp.stats.percentile(summary[:, 0, :], 0.5, axis=0)\n",
        "    A_max = tfp.stats.percentile(summary[:, 0, :], 99.5, axis=0)\n",
        "    R_mean = tfp.stats.percentile(summary[:, 1, :], 50, axis=0)\n",
        "    R_min = tfp.stats.percentile(summary[:, 1, :], 0.5, axis=0)\n",
        "    R_max = tfp.stats.percentile(summary[:, 1, :], 99.5, axis=0)\n",
        "    D_mean = tfp.stats.percentile(summary[:, 2, :], 50, axis=0)\n",
        "    D_min = tfp.stats.percentile(summary[:, 2, :], 0.5, axis=0)\n",
        "    D_max = tfp.stats.percentile(summary[:, 2, :], 99.5, axis=0)\n",
        "\n",
        "    axs[0].set(ylabel=\"Active\", yscale=\"log\")\n",
        "    axs[0].vlines(TRAIN_DAYS,0,np.max(country_data_test[0, :]), linestyles='dashed')\n",
        "    axs[0].plot(x_axis, country_data_test[0, :], \"black\")\n",
        "    axs[0].plot(x_axis, A_mean, \"b\")\n",
        "    axs[0].fill_between(\n",
        "        x_axis,\n",
        "        A_min,\n",
        "        A_max,\n",
        "        alpha=0.3,\n",
        "        color=\"b\",\n",
        "    )\n",
        "    \n",
        "\n",
        "    axs[1].set(ylabel=\"Recovered\", yscale=\"log\")\n",
        "    axs[1].vlines(TRAIN_DAYS,0,np.max(country_data_test[1, :]), linestyles='dashed')\n",
        "    axs[1].plot(x_axis, country_data_test[1, :], \"black\")\n",
        "    axs[1].plot(x_axis, R_mean, \"g\")\n",
        "    axs[1].fill_between(\n",
        "        x_axis,\n",
        "        R_min,\n",
        "        R_max,\n",
        "        alpha=0.3,\n",
        "        color=\"g\",\n",
        "    )\n",
        "\n",
        "    axs[2].set(ylabel=\"Deaths\", yscale=\"log\")\n",
        "    axs[2].vlines(TRAIN_DAYS,0,np.max(country_data_test[2, :]), linestyles='dashed')\n",
        "    axs[2].plot(x_axis, country_data_test[2, :], \"black\")\n",
        "    axs[2].plot(x_axis, D_mean, \"r\")\n",
        "    axs[2].fill_between(\n",
        "        x_axis,\n",
        "        D_min,\n",
        "        D_max,\n",
        "        alpha=0.3,\n",
        "        color=\"r\",\n",
        "    )\n",
        "\n",
        "    fig.savefig(\"plots.png\", dpi=400)\n",
        "\n",
        "\n",
        "class Covid19Model:\n",
        "    def __init__(self, num_days, num_samples, population, A_0, R_0, D_0):\n",
        "        self.num_days = (\n",
        "            num_days  \n",
        "        )\n",
        "        self.num_samples = (\n",
        "            num_samples \n",
        "        )\n",
        "        self.P = tf.ones(num_samples) * population\n",
        "        self.A_0 = tf.ones(num_samples) * A_0\n",
        "        self.R_0 = tf.ones(num_samples) * R_0\n",
        "        self.D_0 = tf.ones(num_samples) * D_0\n",
        "        self.param_vector = tf.transpose(\n",
        "            tfd.Uniform(\n",
        "                tf.constant([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
        "                tf.constant([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
        "            ).sample(self.num_samples)\n",
        "        )\n",
        "        self.S_store = (\n",
        "            self.P - 2 * self.param_vector[7] * self.A_0 - (self.A_0 + self.R_0 + self.D_0)\n",
        "        )\n",
        "        self.I_store = 2 * self.param_vector[7] * self.A_0\n",
        "        self.A_store = self.A_0\n",
        "        self.R_store = self.R_0\n",
        "        self.D_store = self.D_0\n",
        "        self.Ru_store = tf.zeros(self.num_samples)\n",
        "        self.summary = tf.zeros([self.num_days, 3, self.num_samples])\n",
        "        self.summary = tf.tensor_scatter_nd_add(\n",
        "            self.summary,\n",
        "            [[0, 0], [0, 1], [0, 2]],\n",
        "            tf.stack([self.A_store, self.R_store, self.D_store]),\n",
        "        )\n",
        "        # order of params: alpha_0, alpha, n, beta, gamma, delta, eta, kappa\n",
        "\n",
        "        self.nu = tf.constant(\n",
        "            [\n",
        "                [-1, 1, 0, 0, 0, 0],\n",
        "                [0, -1, 1, 0, 0, 0],\n",
        "                [0, 0, -1, 1, 0, 0],\n",
        "                [0, 0, -1, 0, 1, 0],\n",
        "                [0, -1, 0, 0, 0, 1],\n",
        "            ],\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "\n",
        "    def one_day(self, i, s, S_store, I_store, A_store, R_store, D_store, Ru_store):\n",
        "        alpha_t = self.param_vector[0] + (\n",
        "            100 * self.param_vector[1]\n",
        "            / (\n",
        "                tf.constant(1.0)\n",
        "                + tf.pow(A_store + R_store + D_store, 2 * self.param_vector[2])\n",
        "            )\n",
        "        )\n",
        "        h_1 = (S_store * I_store / self.P) * alpha_t\n",
        "        h_2 = I_store * self.param_vector[4]\n",
        "        h_3 = A_store * self.param_vector[3]\n",
        "        h_4 = A_store * self.param_vector[5]\n",
        "        h_5 = I_store * self.param_vector[6] * self.param_vector[3]\n",
        "        h = tf.stack([h_1, h_2, h_3, h_4, h_5])\n",
        "        Y_store = tf.clip_by_value(\n",
        "            tf.math.floor(tfd.Normal(loc=h, scale=tf.sqrt(h)).sample()), 0.0, self.P\n",
        "        )\n",
        "        S = tf.clip_by_value(\n",
        "            S_store + tf.tensordot(self.nu[:, 0], Y_store, axes=1), 0.0, self.P\n",
        "        )\n",
        "        I = tf.clip_by_value(\n",
        "            I_store + tf.tensordot(self.nu[:, 1], Y_store, axes=1), 0.0, self.P\n",
        "        )\n",
        "        A = tf.clip_by_value(\n",
        "            A_store + tf.tensordot(self.nu[:, 2], Y_store, axes=1), 0.0, self.P\n",
        "        )\n",
        "        R = tf.clip_by_value(\n",
        "            R_store + tf.tensordot(self.nu[:, 3], Y_store, axes=1), 0.0, self.P\n",
        "        )\n",
        "        D = tf.clip_by_value(\n",
        "            D_store + tf.tensordot(self.nu[:, 4], Y_store, axes=1), 0.0, self.P\n",
        "        )\n",
        "        Ru = tf.clip_by_value(\n",
        "            Ru_store + tf.tensordot(self.nu[:, 5], Y_store, axes=1), 0.0, self.P\n",
        "        )\n",
        "        s = tf.tensor_scatter_nd_add(s, [[i, 0], [i, 1], [i, 2]], tf.stack([A, R, D]))\n",
        "        (S_store, I_store, A_store, R_store, D_store, Ru_store) = (S, I, A, R, D, Ru)\n",
        "        return tf.add(i, 1), s, S_store, I_store, A_store, R_store, D_store, Ru_store\n",
        "\n",
        "    def summary_statistic(self):\n",
        "        i = tf.constant(1)\n",
        "        (\n",
        "            r,\n",
        "            self.summary,\n",
        "            self.S_store,\n",
        "            self.I_store,\n",
        "            self.A_store,\n",
        "            self.R_store,\n",
        "            self.D_store,\n",
        "            self.Ru_store,\n",
        "        ) = tf.while_loop(\n",
        "            lambda i, *_: i < self.num_days,\n",
        "            self.one_day,\n",
        "            [\n",
        "                i,\n",
        "                self.summary,\n",
        "                self.S_store,\n",
        "                self.I_store,\n",
        "                self.A_store,\n",
        "                self.R_store,\n",
        "                self.D_store,\n",
        "                self.Ru_store,\n",
        "            ],\n",
        "            parallel_iterations=1,\n",
        "        )\n",
        "        summary = self.summary\n",
        "        self.reset()\n",
        "        return summary\n",
        "\n",
        "    def reset(self):\n",
        "        self.Y_store = tf.ones(\n",
        "            [5, self.num_samples], dtype=tf.float32\n",
        "        )  # !!changed_by_AT!! * -float('inf'))\n",
        "        self.S_store = (\n",
        "            self.P - 2 * self.param_vector[7] * self.A_0 - (self.A_0 + self.R_0 + self.D_0)\n",
        "        )\n",
        "        self.I_store = 2 * self.param_vector[7] * self.A_0\n",
        "        self.A_store = self.A_0\n",
        "        self.R_store = self.R_0\n",
        "        self.D_store = self.D_0\n",
        "        self.Ru_store = tf.zeros(self.num_samples)\n",
        "        self.summary = tf.zeros([self.num_days, 3, self.num_samples])\n",
        "        self.summary = tf.tensor_scatter_nd_add(\n",
        "            self.summary,\n",
        "            [[0, 0], [0, 1], [0, 2]],\n",
        "            tf.stack([self.A_store, self.R_store, self.D_store]),\n",
        "        )\n",
        "        self.param_vector = tf.transpose(\n",
        "            tfd.Uniform(\n",
        "                tf.constant([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
        "                tf.constant([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
        "            ).sample(self.num_samples)\n",
        "        )\n",
        "\n",
        "\n",
        "def collect_summaries(param_vector):\n",
        "    model = Covid19Model(\n",
        "        num_days=TEST_DAYS,\n",
        "        num_samples=param_vector.shape[0],\n",
        "        population=tf.constant(60.36e6, dtype=tf.float32),\n",
        "        A_0=country_data_test[0, 0],\n",
        "        R_0=country_data_test[1, 0],\n",
        "        D_0=country_data_test[2, 0],\n",
        "    )\n",
        "    model.param_vector = tf.transpose(param_vector)\n",
        "    print(model.param_vector.shape)\n",
        "    summary = tf.transpose(model.summary_statistic(), perm=[2, 1, 0])\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8GcGzDJY4q1"
      },
      "source": [
        "# GPU-optimized simulation code for epidemology model\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def build_graph(tolerance, param_vector):\n",
        "    num_days = tf.cast(country_data_train.shape[1], tf.int32)\n",
        "    P = tf.ones(num_samples) * population\n",
        "    A_0 = tf.ones(num_samples) * country_data_train[0, 0]\n",
        "    R_0 = tf.ones(num_samples) * country_data_train[1, 0]\n",
        "    D_0 = tf.ones(num_samples) * country_data_train[2, 0]\n",
        "\n",
        "    summary = tf.zeros([num_days, 3, num_samples])\n",
        "\n",
        "    nu = tf.constant(\n",
        "        [\n",
        "            [-1, 1, 0, 0, 0, 0],\n",
        "            [0, -1, 1, 0, 0, 0],\n",
        "            [0, 0, -1, 1, 0, 0],\n",
        "            [0, 0, -1, 0, 1, 0],\n",
        "            [0, -1, 0, 0, 0, 1],\n",
        "        ],\n",
        "        dtype=tf.float32,\n",
        "    )\n",
        "\n",
        "    S_store = P - 2 * param_vector[7] * A_0 - (A_0 + R_0 + D_0)\n",
        "    I_store = 2 * param_vector[7] * A_0\n",
        "    A_store = A_0\n",
        "    R_store = R_0\n",
        "    D_store = D_0\n",
        "    Ru_store = tf.zeros(num_samples)\n",
        "\n",
        "    summary = tf.tensor_scatter_nd_add(\n",
        "        summary, [[0, 0], [0, 1], [0, 2]], tf.stack([A_store, R_store, D_store])\n",
        "    )\n",
        "\n",
        "    def body(i, s, S, I, A, R, D, Ru):\n",
        "        U = A + R + D\n",
        "        alpha_t = param_vector[0] + (\n",
        "            100 * param_vector[1] / (tf.constant(1.0) + tf.pow(U, 2 * param_vector[2]))\n",
        "        )\n",
        "        h_1 = (S * I / P) * alpha_t\n",
        "        h_2 = I * param_vector[4]\n",
        "        h_3 = A * param_vector[3]\n",
        "        h_4 = A * param_vector[5]\n",
        "        h_5 = I * param_vector[6] * param_vector[3]\n",
        "        h = tf.stack([h_1, h_2, h_3, h_4, h_5])\n",
        "        Y_store = tf.clip_by_value(\n",
        "            tf.math.floor(tfd.Normal(loc=h, scale=tf.sqrt(h)).sample()), 0.0, P\n",
        "        )\n",
        "\n",
        "        m = tf.matmul(tf.transpose(nu), Y_store)\n",
        "\n",
        "        S = tf.clip_by_value(S + m[0, :], 0.0, P)\n",
        "        I = tf.clip_by_value(I + m[1, :], 0.0, P)\n",
        "        A = tf.clip_by_value(A + m[2, :], 0.0, P)\n",
        "        R = tf.clip_by_value(R + m[3, :], 0.0, P)\n",
        "        D = tf.clip_by_value(D + m[4, :], 0.0, P)\n",
        "        Ru = tf.clip_by_value(Ru + m[5, :], 0.0, P)\n",
        "\n",
        "        s = tf.tensor_scatter_nd_add(s, [[i, 0], [i, 1], [i, 2]], tf.stack([A, R, D]))\n",
        "\n",
        "        return i + 1, s, S, I, A, R, D, Ru\n",
        "\n",
        "    init_idx = tf.zeros([], dtype=tf.int32) + 1\n",
        "    i, summary, *_ = tf.while_loop(\n",
        "        lambda i, *_: i < num_days,\n",
        "        body,\n",
        "        [init_idx, summary, S_store, I_store, A_store, R_store, D_store, Ru_store],\n",
        "    )\n",
        "\n",
        "    t_summary = tf.transpose(summary, perm=[2, 1, 0])\n",
        "    # Normalized Distances\n",
        "    distances = tf.norm(\n",
        "        tf.broadcast_to(\n",
        "            country_data_train / tf.reduce_max(country_data_train,axis=1, keepdims=True),\n",
        "            tf.constant(\n",
        "                [num_samples, country_data_train.shape[0], country_data_train.shape[1]],\n",
        "                dtype=tf.int32,\n",
        "            ) ,\n",
        "        )\n",
        "        - t_summary / tf.reduce_max(country_data_train,axis=1, keepdims=True),\n",
        "        axis=2,\n",
        "    )\n",
        "\n",
        "    # # Un-normalized Distances\n",
        "    # distances = tf.norm(\n",
        "    #     tf.broadcast_to(\n",
        "    #         country_data_train,\n",
        "    #         tf.constant(\n",
        "    #             [num_samples, country_data_train.shape[0], country_data_train.shape[1]],\n",
        "    #             dtype=tf.int32,\n",
        "    #         ) ,\n",
        "    #     )\n",
        "    #     - t_summary,\n",
        "    #     axis=2,\n",
        "    # )\n",
        "\n",
        "    reduced_distances = tf.reduce_sum(distances, axis=1)\n",
        "    acceptance_vector = reduced_distances <= tolerance\n",
        "    num_accepted_samples = tf.reduce_sum(\n",
        "        tf.cast(acceptance_vector, dtype=tf.float32), name=\"num_accepted_samples\"\n",
        "    )\n",
        "    return num_accepted_samples, acceptance_vector, param_vector, reduced_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPq-lb2tYFC1"
      },
      "source": [
        "## Baseline Inference Method: P-ABC-SMC MCMC\n",
        "\n",
        "As a baseline, we consider the state-of-the-art Adaptive Approximate Bayesian Computation with Sequential Monte Carlo (ABC-SMC) inference with MCMC tuned steps. This is the inference method used by the original authours of the epidemology model.\n",
        "The algorithm is parallelized to obtain the best possible performance on GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4EHR2WzYI4Q"
      },
      "source": [
        "## MCMC version\n",
        "\n",
        "NUM_ADAPT_STEPS = 0.1 * max_runs\n",
        "per_trial_tolerances_mcmc = []\n",
        "per_trial_runs_mcmc = []\n",
        "for _ in range(NUM_TRIALS):\n",
        "  # Time it\n",
        "  initial_tolerance = 1e9\n",
        "  target_tolerance = 0.1\n",
        "  alpha = 0.5\n",
        "  print(\"Running...\")\n",
        "  samples_target = 1000\n",
        "  samples_collected = 0\n",
        "  num_runs = 0\n",
        "  start_time = time.time()\n",
        "  tolerance = initial_tolerance\n",
        "  prev_stage_samples = []\n",
        "  prev_stage_distances = []\n",
        "  stage = 0\n",
        "  step_size_multiplier = 1\n",
        "  step_size = 0.1\n",
        "  num_accepted_samples = 0\n",
        "  acceptance_ratio = 1\n",
        "  per_stage_tolerances_mcmc = []\n",
        "  per_stage_runs_mcmc = []\n",
        "  \n",
        "  # # Performance Profiling\n",
        "  # sim_time = 0\n",
        "\n",
        "  while tolerance >= target_tolerance and num_runs <= max_runs: # and acceptance_ratio > 0.02:\n",
        "      accepted_param_vector = []\n",
        "      accepted_distances = []\n",
        "      samples_collected = 0\n",
        "      for _ in range(max_runs):\n",
        "          # print(prev_stage_samples)\n",
        "          if stage == 0: \n",
        "              prev_stage_samples = tfd.Uniform(\n",
        "                  tf.constant([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
        "                  tf.constant([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
        "              ).sample(num_samples)\n",
        "              resampled_params = tf.transpose(prev_stage_samples)\n",
        "              # stage += 1\n",
        "\n",
        "          else:\n",
        "              # resmaple params from last stage and do MCMC acceptance\n",
        "              resampled_params = tf.transpose(\n",
        "                  tf.gather(\n",
        "                      prev_stage_samples,\n",
        "                      tf.random.categorical([tolerance - prev_stage_distances], num_samples)[0],\n",
        "                      axis=0,\n",
        "                  )\n",
        "              )\n",
        "\n",
        "          # Metropolis-Hastings Acceptance\n",
        "          \n",
        "          if num_runs < NUM_ADAPT_STEPS:\n",
        "            # step size tuning\n",
        "            step_size = tf.constant(step_size * step_size_multiplier, dtype=tf.float32)\n",
        "\n",
        "          # proposed_param_vector = tf.clip_by_value(\n",
        "          #     tfd.Normal(resampled_params, scale=step_size).sample(), 0, 1\n",
        "          # )\n",
        "\n",
        "          # Truncated Gaussian Random Walk\n",
        "          proposed_param_vector = tfd.TruncatedNormal(loc=resampled_params, scale=step_size, low=0., high=1.).sample()\n",
        "\n",
        "          fwd_log_prob = tf.reduce_sum(tfd.TruncatedNormal(loc=resampled_params, scale=step_size, low=0., high=1.).log_prob(proposed_param_vector), axis=0)\n",
        "          rev_log_prob = tf.reduce_sum(tfd.TruncatedNormal(loc=proposed_param_vector, scale=step_size, low=0., high=1.).log_prob(resampled_params), axis=0)\n",
        "          mcmc_prob = tf.clip_by_value(tf.exp(fwd_log_prob + rev_log_prob), 0, 1)\n",
        "          random_nos = tf.random.uniform( [num_samples], minval=0, maxval=1)\n",
        "          mcmc_acceptance_vector = mcmc_prob >= random_nos\n",
        "          mcmc_rejectance_vector = mcmc_prob < random_nos\n",
        "\n",
        "          # # Acceptance ratio old version\n",
        "          # acceptance_ratio = (acceptance_ratio + tf.math.count_nonzero(mcmc_acceptance_vector, dtype=tf.float32)/mcmc_acceptance_vector.shape[0]) / 2\n",
        "\n",
        "          target_acceptance_ratio = tf.constant(0.234, dtype=tf.float32)\n",
        "\n",
        "          if num_runs < NUM_ADAPT_STEPS:\n",
        "            # Compute update to step size\n",
        "            step_size_multiplier = tf.constant(tf.math.exp((acceptance_ratio - target_acceptance_ratio) /((1.0 - target_acceptance_ratio) * (num_accepted_samples + 1))), dtype=tf.float32)\n",
        "\n",
        "          param_vector = tf.transpose(tf.concat([tf.boolean_mask(tf.transpose(proposed_param_vector), mcmc_acceptance_vector), tf.boolean_mask(tf.transpose(resampled_params), mcmc_rejectance_vector)], axis=0))\n",
        "\n",
        "          # Perform one batch of simulation        \n",
        "          # sim_time_accumulator = time.time() # for profiling\n",
        "          num_accepted_samples, acceptance_vector, param_vector, distances = build_graph(\n",
        "              tolerance,\n",
        "              param_vector,\n",
        "          )\n",
        "          # sim_time += time.time() - sim_time_accumulator # for profiling\n",
        "          # Acceptance ratio new version\n",
        "          acceptance_ratio = (acceptance_ratio + num_accepted_samples/num_samples) / 2\n",
        "\n",
        "          # Post processing\n",
        "          samples_collected += num_accepted_samples\n",
        "          samples_blah = tf.boolean_mask(tf.transpose(param_vector), acceptance_vector)\n",
        "          samples_distances = tf.boolean_mask(distances, acceptance_vector)\n",
        "          for i in range(len(samples_blah)):\n",
        "              accepted_param_vector.append(samples_blah[i])\n",
        "              accepted_distances.append(samples_distances[i])\n",
        "          num_runs += 1\n",
        "          if samples_collected >= samples_target:\n",
        "              if tolerance <= target_tolerance or num_runs >= max_runs - 1: \n",
        "                  accepted_param_vector = tf.stack(accepted_param_vector)\n",
        "                  accepted_distances = tf.stack(accepted_distances)\n",
        "                  break\n",
        "              prev_stage_distances, accepted_indices = tf.math.top_k(\n",
        "                  -1 * tf.stack(accepted_distances), int(samples_target * alpha),\n",
        "              )\n",
        "              prev_stage_distances = -prev_stage_distances\n",
        "              prev_stage_samples = tf.gather(\n",
        "                  tf.stack(accepted_param_vector), accepted_indices\n",
        "              )\n",
        "              tolerance = tf.math.top_k(prev_stage_distances)[0]\n",
        "              stage += 1\n",
        "              break\n",
        "      print(f\"stage: {stage}, tolerance: {tolerance}, number of runs: {num_runs}, acceptance ratio: {acceptance_ratio}\")\n",
        "      per_stage_tolerances_mcmc.append(tolerance)\n",
        "      per_stage_runs_mcmc.append(num_runs)\n",
        "  per_trial_tolerances_mcmc.append(per_stage_tolerances_mcmc)\n",
        "  per_trial_runs_mcmc.append(per_stage_runs_mcmc)\n",
        "  stages_mcmc = stage+1\n",
        "  end_time = time.time()\n",
        "  print(\"Completed in {0:.3f} seconds\\n\".format(end_time - start_time))\n",
        "  print(f\"Samples collected: {samples_collected}\")\n",
        "  print(f\"Number of runs: {num_runs}\")\n",
        "  print(\n",
        "      \"Time per run: {0:.3f} milliseconds\\n\".format(\n",
        "          1e3 * (end_time - start_time) / num_runs\n",
        "      )\n",
        "  )\n",
        "  # print(\n",
        "  #     \"Time spent in simulation: {0:.3f} milliseconds\\n\".format(\n",
        "  #         1e3 * (sim_time) / num_runs\n",
        "  #     )\n",
        "  # )\n",
        "  print(f\"Final tolerance: {tolerance[0]}\")\n",
        "\n",
        "  # Plot results\n",
        "  output_summary = collect_summaries(tf.constant(tf.stack(accepted_param_vector)))\n",
        "  plot_summary(output_summary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT8u5P8TYRyK"
      },
      "source": [
        "## Proposed Inference Method: P-ABC-SMC BDSS\n",
        "\n",
        "In this proposed method, the MCMC based step sized tuning is replaced by step sizes sampled from a Beta distribution, whose shape is modified by the progress of the ABC-SMC process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CFsoYEspYVMS"
      },
      "source": [
        "per_trial_tolerances_prop = []\n",
        "per_trial_runs_prop = []\n",
        "for trial in range(NUM_TRIALS):\n",
        "  print(f'Trial number {trial + 1}')\n",
        "  # Time it\n",
        "  initial_tolerance = 1e9\n",
        "  target_tolerance = 0.5\n",
        "  alpha = 0.5\n",
        "  print(\"Running...\")\n",
        "  # max_runs = 50\n",
        "  samples_target = 1000\n",
        "  samples_collected = 0\n",
        "  num_runs = 0\n",
        "  start_time = time.time()\n",
        "  tolerance = initial_tolerance\n",
        "  prev_stage_samples = []\n",
        "  prev_stage_distances = []\n",
        "  per_stage_tolerances_prop = []\n",
        "  per_stage_runs_prop = []\n",
        "  stage = 0\n",
        "\n",
        "  # # Performance Profiling\n",
        "  # sim_time = 0\n",
        "\n",
        "  while tolerance >= target_tolerance and num_runs <= max_runs:\n",
        "      accepted_param_vector = []\n",
        "      accepted_distances = []\n",
        "      samples_collected = 0\n",
        "      for _ in range(max_runs):\n",
        "          \n",
        "          if stage == 1:\n",
        "            initial_tolerance = tolerance\n",
        "          \n",
        "          if stage == 0:\n",
        "            prev_stage_samples = tfd.Uniform(\n",
        "                tf.constant([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
        "                tf.constant([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
        "            ).sample(num_samples)\n",
        "            resampled_params = tf.transpose(prev_stage_samples)\n",
        "          else:\n",
        "            resampled_params = tf.transpose(\n",
        "                tf.gather(\n",
        "                    prev_stage_samples,\n",
        "                    tf.random.categorical([tolerance - prev_stage_distances], num_samples)[0],\n",
        "                    axis=0,\n",
        "                )\n",
        "            )\n",
        "\n",
        "          # non-MCMC acceptance\n",
        "          \n",
        "          # Original BDSS tuning\n",
        "          step_size = tfp.distributions.Beta(tolerance/initial_tolerance, 2 * (stage + 1))\n",
        "          \n",
        "          # # Mode Concentration reparameterization\n",
        "          # mode = tolerance/initial_tolerance\n",
        "          # concentration = 1 * (stage + 1) # 10\n",
        "          # alpha_param = mode * (concentration - 2) + 1\n",
        "          # beta = (1 - mode) * (concentration - 2) + 1\n",
        "          # step_size = tfp.distributions.Beta(alpha_param, beta)\n",
        "\n",
        "          # # Mean Variance reparameterization\n",
        "          # mean = tolerance/initial_tolerance\n",
        "          # variance = 1/(stage+1)\n",
        "          # alpha_param = mean * variance\n",
        "          # beta = (1 - mean) * variance\n",
        "          # step_size = tfp.distributions.Beta(alpha_param, beta)\n",
        "\n",
        "          # Truncated Gaussian Random Walk Perturbation\n",
        "          param_vector = tfd.TruncatedNormal(loc=resampled_params, scale=tf.clip_by_value(tf.squeeze(step_size.sample(resampled_params.shape)), 1e-30, 1), low=0.0, high=1.0).sample()\n",
        "          \n",
        "          # sim_time_accumulator = time.time() # for profiling          \n",
        "\n",
        "          num_accepted_samples, acceptance_vector, param_vector, distances = build_graph(\n",
        "              tolerance,\n",
        "              param_vector,\n",
        "          )\n",
        "\n",
        "          # sim_time += time.time() - sim_time_accumulator # for profiling\n",
        "\n",
        "          samples_collected += num_accepted_samples\n",
        "          samples_blah = tf.boolean_mask(tf.transpose(param_vector), acceptance_vector)\n",
        "          samples_distances = tf.boolean_mask(distances, acceptance_vector)\n",
        "          for i in range(len(samples_blah)):\n",
        "              accepted_param_vector.append(samples_blah[i])\n",
        "              accepted_distances.append(samples_distances[i])\n",
        "          num_runs += 1\n",
        "          if samples_collected >= samples_target:\n",
        "              if tolerance <= target_tolerance or num_runs >= max_runs - 1:\n",
        "                  accepted_param_vector = tf.stack(accepted_param_vector)\n",
        "                  accepted_distances = tf.stack(accepted_distances)\n",
        "                  break\n",
        "              prev_stage_distances, accepted_indices = tf.math.top_k(\n",
        "                  -1 * tf.stack(accepted_distances), int(samples_target * alpha),\n",
        "              )\n",
        "              prev_stage_distances = -prev_stage_distances\n",
        "              prev_stage_samples = tf.gather(\n",
        "                  tf.stack(accepted_param_vector), accepted_indices\n",
        "              )\n",
        "              tolerance = tf.math.top_k(prev_stage_distances)[0]\n",
        "              stage += 1\n",
        "              break\n",
        "      print(f\"stage: {stage}, tolerance: {tolerance.numpy()[0]}, number of runs: {num_runs}\")\n",
        "      per_stage_tolerances_prop.append(tolerance)\n",
        "      per_stage_runs_prop.append(num_runs)\n",
        "  per_trial_tolerances_prop.append(per_stage_tolerances_prop)\n",
        "  per_trial_runs_prop.append(per_stage_runs_prop)\n",
        "  stages_prop = stage+1\n",
        "  end_time = time.time()\n",
        "  # print(accepted_param_vector)\n",
        "  print(\"Completed in {0:.3f} seconds\\n\".format(end_time - start_time))\n",
        "  print(f\"Samples collected: {samples_collected}\")\n",
        "  print(f\"Number of runs: {num_runs}\")\n",
        "  print(\n",
        "      \"Time per run: {0:.3f} milliseconds\\n\".format(\n",
        "          1e3 * (end_time - start_time) / num_runs\n",
        "      )\n",
        "  )\n",
        "  # print(\n",
        "  #     \"Time spent in simulation: {0:.3f} milliseconds\\n\".format(\n",
        "  #         1e3 * (sim_time) / num_runs\n",
        "  #     )\n",
        "  # )\n",
        "  print(f\"Final tolerance: {tolerance[0]}\")\n",
        "\n",
        "  output_summary = collect_summaries(tf.constant(tf.stack(accepted_param_vector)))\n",
        "  plot_summary(output_summary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5qNiboER3zD"
      },
      "source": [
        "!mkdir plot_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zj13gajIeZ1"
      },
      "source": [
        "SAVE_DATA = True\n",
        "\n",
        "plt.figure(dpi=400)\n",
        "for trial in range(NUM_TRIALS):\n",
        "  plt.plot(np.arange(len(per_trial_tolerances_mcmc[trial])),np.squeeze(np.array(per_trial_tolerances_mcmc[trial])), 'b')\n",
        "  plt.plot(np.arange(len(per_trial_tolerances_prop[trial])),np.squeeze(np.array(per_trial_tolerances_prop[trial])), 'r')\n",
        "  if SAVE_DATA:\n",
        "    np.save(f'plot_data/per_trial_tolerances_mcmc{trial}', np.squeeze(np.array(per_trial_tolerances_mcmc[trial])))\n",
        "    np.save(f'plot_data/per_trial_tolerances_prop{trial}', np.squeeze(np.array(per_trial_tolerances_prop[trial])))\n",
        "\n",
        "plt.plot(np.arange(len(per_trial_tolerances_mcmc[trial])),np.squeeze(np.array(per_trial_tolerances_mcmc[trial])), 'b', label='P-ABC-SMC MCMC')\n",
        "plt.plot(np.arange(len(per_trial_tolerances_prop[trial])),np.squeeze(np.array(per_trial_tolerances_prop[trial])), 'r', label='P-ABC-SMC BDSS')\n",
        "\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('Number of stages')\n",
        "plt.ylabel('Tolerance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MgmtwAYUWXS"
      },
      "source": [
        "SAVE_DATA = True\n",
        "\n",
        "plt.figure(dpi=400)\n",
        "for trial in range(NUM_TRIALS):\n",
        "  plt.plot(np.arange(len(per_trial_runs_mcmc[trial])),np.squeeze(np.array(per_trial_runs_mcmc[trial])), 'b')\n",
        "  plt.plot(np.arange(len(per_trial_runs_prop[trial])),np.squeeze(np.array(per_trial_runs_prop[trial])), 'r')\n",
        "  if SAVE_DATA:\n",
        "    np.save(f'plot_data/per_trial_runs_mcmc{trial}', np.squeeze(np.array(per_trial_runs_mcmc[trial])))\n",
        "    np.save(f'plot_data/per_trial_runs_prop{trial}', np.squeeze(np.array(per_trial_runs_prop[trial])))\n",
        "\n",
        "plt.plot(np.arange(len(per_trial_runs_mcmc[trial])),np.squeeze(np.array(per_trial_runs_mcmc[trial])), 'b', label='ABC-SMC MCMC')\n",
        "plt.plot(np.arange(len(per_trial_runs_prop[trial])),np.squeeze(np.array(per_trial_runs_prop[trial])), 'r', label='ABC-SMC NEW')\n",
        "\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('Number of stages')\n",
        "plt.ylabel('Number of simulation runs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_1KhKQL0ewr"
      },
      "source": [
        "plt.figure(dpi=300)\n",
        "plt.ylim([0,1000000])\n",
        "# plt.xlim([0,1000])\n",
        "# plt.xscale('log')\n",
        "# plt.yscale('log')\n",
        "for trial in range(NUM_TRIALS):\n",
        "  plt.plot(np.squeeze(np.array(per_trial_runs_mcmc[trial])), np.squeeze(np.array(per_trial_tolerances_mcmc[trial])), 'b')\n",
        "  plt.plot(np.squeeze(np.array(per_trial_runs_prop[trial])),np.squeeze(np.array(per_trial_tolerances_prop[trial])), 'r')\n",
        "\n",
        "plt.plot(np.squeeze(np.array(per_trial_runs_mcmc[trial])), np.squeeze(np.array(per_trial_tolerances_mcmc[trial])), 'b', label='ABC-SMC MCMC')\n",
        "plt.plot(np.squeeze(np.array(per_trial_runs_prop[trial])),np.squeeze(np.array(per_trial_tolerances_prop[trial])), 'r', label='ABC-SMC NEW')\n",
        "\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('Number of simulation runs')\n",
        "plt.ylabel('Tolerance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hebaIMsRjpa"
      },
      "source": [
        "!zip -r plot_data.zip plot_data/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}